# src/audio/bpm_processing.py

"""
BPM-aware audio processing for EDM vocals
Works with neural voice cloning output to add musical timing and effects
"""

import numpy as np
import soundfile as sf
from typing import Optional, Tuple
import logging

logger = logging.getLogger(__name__)

def calculate_beat_duration(bpm: float) -> float:
    """Calculate duration of one beat in seconds"""
    return 60.0 / bpm

def apply_bpm_effects(
    audio: np.ndarray,
    sample_rate: int,
    bpm: Optional[float] = None,
    sync_to_beat: bool = False,
    quantize_timing: bool = False,
    add_reverb: bool = False,
    reverb_amount: float = 0.3,
    add_delay: bool = False,
    delay_time: float = 0.125
) -> np.ndarray:
    """
    Apply BPM-aware effects to audio generated by neural voice cloning
    
    Args:
        audio: Input audio array
        sample_rate: Audio sample rate
        bpm: Target BPM (if None, no BPM processing)
        sync_to_beat: Align audio to beat grid
        quantize_timing: Quantize audio timing
        add_reverb: Add reverb effect
        reverb_amount: Reverb intensity (0.0-1.0)
        add_delay: Add delay effect
        delay_time: Delay time in seconds
        
    Returns:
        Processed audio array
    """
    processed = audio.copy()
    
    try:
        # BPM-based processing
        if bpm and bpm > 0:
            beat_duration = calculate_beat_duration(bpm)
            logger.info(f"Processing audio for BPM: {bpm} (beat duration: {beat_duration:.3f}s)")
            
            if sync_to_beat:
                processed = sync_audio_to_beat(processed, sample_rate, beat_duration)
            
            if quantize_timing:
                processed = quantize_audio_timing(processed, sample_rate, beat_duration)
        
        # Add reverb effect
        if add_reverb:
            processed = add_simple_reverb(processed, sample_rate, reverb_amount)
        
        # Add delay effect
        if add_delay:
            # Calculate delay in samples
            if bpm and bpm > 0:
                # Use musical delay (1/8, 1/4, etc.)
                beat_duration = calculate_beat_duration(bpm)
                delay_samples = int(delay_time * beat_duration * sample_rate)
            else:
                # Use absolute delay time
                delay_samples = int(delay_time * sample_rate)
            
            processed = add_simple_delay(processed, delay_samples)
        
        # Normalize to prevent clipping
        processed = normalize_audio(processed)
        
        return processed
        
    except Exception as e:
        logger.error(f"BPM processing failed: {e}")
        return audio  # Return original if processing fails

def sync_audio_to_beat(audio: np.ndarray, sample_rate: int, beat_duration: float) -> np.ndarray:
    """Align audio timing to beat grid"""
    try:
        # Simple implementation: pad/trim to nearest beat boundary
        audio_duration = len(audio) / sample_rate
        beats = round(audio_duration / beat_duration)
        target_duration = beats * beat_duration
        target_samples = int(target_duration * sample_rate)
        
        if target_samples > len(audio):
            # Pad with silence
            padding = target_samples - len(audio)
            synced = np.pad(audio, (0, padding), mode='constant')
        else:
            # Trim
            synced = audio[:target_samples]
        
        logger.info(f"Synced audio to {beats} beats ({target_duration:.3f}s)")
        return synced
        
    except Exception as e:
        logger.error(f"Beat sync failed: {e}")
        return audio

def quantize_audio_timing(audio: np.ndarray, sample_rate: int, beat_duration: float) -> np.ndarray:
    """Quantize audio timing to musical grid"""
    try:
        # Simple quantization: align onset to nearest 16th note
        sixteenth_duration = beat_duration / 4  # 16th note
        sixteenth_samples = int(sixteenth_duration * sample_rate)
        
        # Find audio onset (first significant amplitude)
        threshold = 0.1 * np.max(np.abs(audio))
        onset_idx = 0
        
        for i, sample in enumerate(audio):
            if abs(sample) > threshold:
                onset_idx = i
                break
        
        # Quantize onset to nearest 16th note
        onset_beat = onset_idx / sixteenth_samples
        quantized_beat = round(onset_beat)
        quantized_idx = quantized_beat * sixteenth_samples
        
        # Shift audio to quantized position
        shift = quantized_idx - onset_idx
        
        if shift > 0:
            # Add silence at beginning
            quantized = np.pad(audio, (int(shift), 0), mode='constant')
        elif shift < 0:
            # Trim from beginning
            quantized = audio[int(-shift):]
        else:
            quantized = audio
        
        logger.info(f"Quantized timing by {shift} samples")
        return quantized
        
    except Exception as e:
        logger.error(f"Timing quantization failed: {e}")
        return audio

def add_simple_reverb(audio: np.ndarray, sample_rate: int, amount: float = 0.3) -> np.ndarray:
    """Add simple reverb effect"""
    try:
        # Simple reverb using multiple delays
        delays = [0.03, 0.05, 0.07, 0.11]  # Reverb delay times
        gains = [0.7, 0.5, 0.4, 0.3]       # Reverb gains
        
        reverb = np.zeros_like(audio)
        
        for delay_time, gain in zip(delays, gains):
            delay_samples = int(delay_time * sample_rate)
            if delay_samples < len(audio):
                delayed = np.pad(audio, (delay_samples, 0), mode='constant')[:len(audio)]
                reverb += delayed * gain * amount
        
        # Mix with original
        return audio + reverb
        
    except Exception as e:
        logger.error(f"Reverb processing failed: {e}")
        return audio

def add_simple_delay(audio: np.ndarray, delay_samples: int, feedback: float = 0.4, mix: float = 0.3) -> np.ndarray:
    """Add simple delay effect"""
    try:
        if delay_samples <= 0 or delay_samples >= len(audio):
            return audio
        
        delayed = np.zeros_like(audio)
        
        # Create delay line with feedback
        for i in range(delay_samples, len(audio)):
            delayed[i] = audio[i - delay_samples] + delayed[i - delay_samples] * feedback
        
        # Mix with original
        return audio + delayed * mix
        
    except Exception as e:
        logger.error(f"Delay processing failed: {e}")
        return audio

def normalize_audio(audio: np.ndarray, target_level: float = 0.95) -> np.ndarray:
    """Normalize audio to prevent clipping"""
    try:
        max_val = np.max(np.abs(audio))
        if max_val > 0:
            return audio * (target_level / max_val)
        return audio
    except Exception:
        return audio

def estimate_bpm_from_text(text: str) -> Optional[float]:
    """
    Estimate appropriate BPM based on text content and length
    Useful for automatic BPM selection
    """
    try:
        # Simple heuristic based on text characteristics
        words = text.split()
        word_count = len(words)
        
        # Shorter phrases -> faster BPM, longer phrases -> slower BPM
        if word_count <= 3:
            return 140.0  # Fast EDM
        elif word_count <= 8:
            return 128.0  # Standard EDM
        elif word_count <= 15:
            return 120.0  # Moderate
        else:
            return 110.0  # Slower for long phrases
            
    except Exception:
        return 128.0  # Default EDM BPM

# Convenience functions for common EDM BPMs
def get_edm_bpm_presets() -> dict:
    """Get common EDM BPM presets"""
    return {
        "house": 128.0,
        "techno": 130.0,
        "trance": 138.0,
        "dubstep": 140.0,
        "trap": 150.0,
        "drum_and_bass": 174.0
    }